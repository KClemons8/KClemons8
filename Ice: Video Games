#!/usr/bin/env python
# coding: utf-8

# ## Which Game succeeds?
# 
# You work for the online store Ice, which sells video games all over the world. User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. You need to identify patterns that determine whether a game succeeds or not. This will allow you to spot potential big winners and plan advertising campaigns.

# The purpose of this project is to look at the dataset and determine if a game succeeds. To detemrine this, the dataframe provides reviews, and game sales. The two combined will help one detemine if, one : the game sold enough copies, and two: what people thing of the game, good or bad. The data also provides the genre of the game, so you can seperate the data by genre since different genres produce different results. The years the games were relaesed will be looked at also, and determine if the data from every period is significant. The steps that will be looked at to get the final conclusion is ficing the data, calculating the total sales, analyzing the data, looking at the years games were released, the platform sales and the genre sales, looking at the different region sales and testing different hypothesis.

# ## Initialization

# In[1]:


# loading all the libraries
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns
from math import factorial
from scipy import stats
from scipy.stats import norm
import datetime


# ## Load Data

# In[2]:


# Load the data files into a DataFrame
try:
    df = pd.read_csv('games.csv')
except:
    df = pd.read_csv('/datasets/games.csv')


# In[3]:


df


# Description of data: 
#  Name of game, Platform game is played on, Year_of_Release, Genre of game , NA_sales (North American sales in USD million), EU_sales (sales in Europe in USD million), JP_sales (sales in Japan in USD million), Other_sales (sales in other countries in USD million), Critic_Score (maximum of 100), User_Score (maximum of 10), Rating (ESRB)

# ## Prepare The Data

# After the data is loaded, the columns were looked at for duplicates, missing data, data converted to proper data types, and columns that need to be renamed. 

# ## Plans

# In[4]:


df.info()


# In[5]:


df.describe()


# In[6]:


df.shape


# In[7]:


df.isna().sum()*100/len(df)


# ## Fix Data 

# Using info and describe functions, it is shown that there are some missing data for Name, Year_of_release, Genre, rating, User_score, and the one with the most missing data is critic_score. .011% of name and genres, 1.62% of year_of_release, 51.31% of critic_score, 40.09% of user_score, and 40.48% of rating missing from the total data set. Also, user_score should be a float and year_of_release should be datetime. Looking at the mean and std in desribe, if the standard deviation is low it means that all of its values are close to the mean, but if the standard deviation is high, the dataset may have menacing anomalies or outliers. Luckily, none of the std's are high. All the column names should be lowercase. Also, there is TBD in user_score. 

# In[8]:


df = df.rename(
    columns = {'Name':'name',
               'Platform':'platform',
               'Year_of_Release':'year_of_release',
               'Genre':'genre',
               'NA_sales':'na_sales',
               'EU_sales':'eu_sales',
               'JP_sales':'jp_sales',
               'Other_sales':'other_sales',
               'Critic_Score':'critic_score',
               'User_Score':'user_score',
               'Rating':'rating',
              }
)
df.head(10)


# In[9]:


df.duplicated().sum()


# In[10]:


df.sort_values(by = 'user_score', ascending = False)
df['user_score'].unique()


# In[11]:


df['user_score'] = df['user_score'].replace(['tbd'],np.nan)
df['user_score'].unique()


# In[12]:


df.sort_values(by='critic_score', ascending=False)
df['critic_score'].unique()


# In[13]:


df['user_score'] = df['user_score'].astype('float')
df.info()


# For TBD, I will add them with the nan's because we do not know what the score is that the users gave us. Then change the data type from an object to a float.

# In[14]:


df.sort_values(by='name', ascending=False)
df['name'].unique()


# In[15]:


df.sort_values(by='year_of_release', ascending=False)
df['year_of_release'].unique()


# In[16]:


df.sort_values(by='genre', ascending=False)
df['genre'].unique()


# In[17]:


df.sort_values(by='rating', ascending=False)
df['rating'].unique()


# In[18]:


df.isna().sum()


# There are 2 names missing, 269 years_of_release missing, 2 genres missing, 8578 critic scores missing, 6701 user scores missing, and 6766 Ratings missing. 

# In[19]:


df['rating'] = df['rating'].fillna('unknown')
df.head(10)


# The missing data for rating was changed to unknown due to not knowing what the rating of the game is.

# In[20]:


df = df.dropna(subset = ['name'])
df.head(10)


# The name rows with missing values was dropped due to the fact that the name of the game is relevant to the analysis of knowing if the game succeeds. Without the name, we dont know the game that succeeded. Getting rid of those two columns also gets rid of the missing data for the genre column. 

# Critic score and user score was kept as NaN since this can be kept as a float and since we do not know the score and if we took the mean/median, it could distort the data.

# Name and year_of_release was grouped together and median taking due to some of the names being repeated, just the games are for different platforms. The rest of the missing values is grouped by genre since certain genres could be released at certain times.

# In[21]:


df = df.dropna(subset = ['year_of_release'])
df.head(10)


# In[22]:


df.info()


# In[23]:


df['year_of_release'] = pd.to_datetime(pd.to_datetime(df['year_of_release'], format = '%Y'))
df['year'] = pd. DatetimeIndex(df['year_of_release']).year
df = df.rename(
    columns = {'year_of_release':'date_of_release',
              'year':'year_of_release',}
)
df


# In[24]:


df.isna().sum()


# The reason for NaN values could be due to not filling in all the information when making the dataset. Also, some of the information could have not been known at the time like the critic_score, the user_score, or the rating. 

# ## Calculate Total Sales

# In[25]:


#total sales (sum of sales in all regions) for each game
na_sales = df['na_sales']
na_sales


# In[26]:


eu_sales = df['eu_sales']
eu_sales


# In[27]:


jp_sales = df['jp_sales']
jp_sales


# In[28]:


other_sales = df['other_sales']
other_sales


# In[29]:


total_sale = na_sales+eu_sales+jp_sales+other_sales
total_sale


# In[30]:


total_sales = total_sale.to_frame()
total_sales
total_sales = total_sales.rename(
    columns = {0:'total_sales',
              }
)
total_sales.reset_index(level = 0, inplace = True)
total_sales


# In[31]:


df.reset_index(level = 0, inplace = True)
df


# In[32]:


df = df.merge(total_sales[['index','total_sales']], on = 'index', how = 'left')
df


# na_sales, eu_sales, jp_sales, an other sales was combined together to get the total sales of all the different games and put into a seperate column.

# ## Analyze The Data

# Research was conducted to determine how many games were released in different years and if every period is significant. Sales variation is looked at to see how they vary from platform to platform, what platforms have the greatest sales, and how long it takes for a game to lose its popularity. The data that is choosing to be relevant is the data that will be analalyed further to build a prognosis for 2017. 

# ## Years games were Released

# In[33]:


release_years = df.groupby(['year_of_release']).size().reset_index()
release_years


# In[34]:


release = df.pivot_table(
    index = 'year_of_release',
    values = 'name',
    aggfunc = 'count'
)
print(release)
release.plot(kind='bar')
plt.ylabel('Amount of Games Released Per Year')


# Looking at the data, there are some time periods that have really low amount of games released for that year. So, any data that is 60 I deemed as not relevant due to the fact that they have such low numbers, it should not affect the data as a whole.

# In[35]:


df = df[df['year_of_release'] > 1993]
df


# ## Platform Sales

# In[36]:


df.sort_values(by='platform', ascending=False)
df['platform'].unique()


# In[37]:


platform_game_total = df.groupby(['platform']).size().reset_index()
platform_game_total


# In[38]:


platform_sales = df.groupby(['platform'])['total_sales'].sum().reset_index()
platform_sales


# PS2 had the highest amount of sales for a platform at 1255.77. The next highest, in order, is X360 at 971.42, PS3 at 939.65, wii at 907.51, DS at 806.12, and PS at 730.86. Then it drops down to 317.85 for GBA, and goes lower from there. With that, the top 5 will be used for any further anlysis. (PS2, X360, PS3, Wii, and DS.) Looking at the boxplot, the difference in sales are different depending on the platform. A lot of platforms have a long range of sales, due to being around for multiple years, but platforms like PCFX has very little sales because it was around for about a year. Looking at the average sales, some of the platforms have lots of total_sales, but the average is pretty low (like the Wii) showing that it started out not popular, then became popular, but died off again. Platforms like the DS has a pretty high average. That s because it started out popular, stayed popular for a while, but then it stopped being popular, dropping the sales. 

# In[39]:


year_platform_sales = df.groupby(['platform','year_of_release'])['total_sales'].sum().reset_index()
year_platform_sales


# In[40]:


plt.figure(figsize=(20,20))
sns.boxplot(x='platform',y='total_sales',data=year_platform_sales, palette='rainbow', showfliers = False)
plt.title("Platforms Total Sales")
plt.show()


# In[41]:


plt.figure(figsize=(20,20))
sns.boxplot(x='year_of_release',y='total_sales',data=year_platform_sales, palette='rainbow', showfliers = False)
plt.title("Platforms Total Sales by Year")
plt.show()


# The year with the most game sales is 2010. All the game sales stayed pretty low untill the 2000's, and the game sales start to drop again after 2010. making 2016 close to 0. 

# ### Platform Years

# In[42]:


data = dict(tuple(year_platform_sales.groupby('platform')))
Xone = data['XOne']
a=data['XOne']
print(a)
print(a.count())


# In[43]:


b=data['XB']
print(b)
print(b.count())


# In[44]:


c=data['X360']
print(c)
print(c.count())


# In[45]:


d=data['WiiU']
print(d)
print(d.count())


# In[46]:


e=data['Wii']
print(e)
print(e.count())


# In[47]:


f=data['WS']
print(f)
print(f.count())


# In[48]:


g=data['TG16']
g


# In[49]:


h=data['SNES']
print(h)
print(h.count())


# In[50]:


i=data['SCD']
print(i)
print(i.count())


# In[51]:


j=data['SAT']
print(j)
print(j.count())


# In[52]:


k=data['PSV']
print(k)
print(k.count())


# In[53]:


l=data['PSP']
print(l)
print(l.count())


# In[54]:


m=data['PS4']
print(m)
print(m.count())


# In[55]:


n=data['PS3']
print(n)
print(n.count())


# In[56]:


o=data['PS2']
print(o)
print(o.count())


# In[57]:


p=data['PS']
print(p)
print(p.count())


# In[58]:


q=data['PCFX']
print(q)


# In[59]:


r=data['PC']
print(r)
print(r.count())


# In[60]:


s=data['NG']
print(s)
print(s.count())


# In[61]:


t=data['NES']
print(t)
print(t.count())


# In[62]:


u=data['N64']
print(u)
print(u.count())


# In[63]:


w=data['GEN']
print(w)
print(w.count())


# In[64]:


x=data['GC']
print(x)
print(x.count())


# In[65]:


y=data['GBA']
print(y)
print(y.count())


# In[66]:


z=data['GB']
print(z)
print(z.count())


# In[67]:


ds=data['DS']
print(ds)
print(ds.count())


# In[68]:


dc=data['DC']
print(dc)
print(dc.count())


# In[69]:


ds3=data['3DS']
print(ds3)
print(ds3.count())


# In[70]:


DO3=data['3DO']
print(DO3)
print(DO3.count())


# In[71]:


total_years = pd.Series([15,2,12,7,10,1,1,1,14,9,8,5,9,12,4,27,13,13,12,4,12,7,62,10,3,13,512,11,4])
total_years.median()


# In[72]:


plt.figure(figsize=(20,20))
sns.boxplot(x='platform',y='year_of_release',data=year_platform_sales, palette='rainbow')
plt.title("Platforms by Year")
plt.show()


# Looking at the years for each game platforms, some of the platforms were popular, but after so many years started to drop towards 0. Some of these platforms include the GC, the N64, the NES, the PS2, the PSP, the SNES, the Wii, and the XB. When looking at all the years for different platforms, it usually takes about 10 years for a platform to fade, with some platforms taking less time or more time. Looking at the boxplot, the platforms do not have a designated time on when they come out. All the platforms overlap in some way. It depends on who makes the platform, and if the previous platform from that company starts making less money, they release a new platform that overlaps the old one. For example, the PS2, PS3, and PS4 are all made from the same company. PS2 overlaps with PS3 a little, but after 5 years the old one fades. Then when PS3 started to fade, PS4 was made and they overlapped a little. The only platform that is showing an increase in sales is the 2600, but it is by a small amount. 

# ### Top 5 platforms

# In[73]:


df_new = df[(df['platform'].isin(['PS2','X360','PS3','Wii','DS']))]
df_new


# In[74]:


platform_sales_5 = df_new.groupby(['platform','year_of_release'])['total_sales'].sum().reset_index()
platform_sales_5


# In[75]:


plt.figure(figsize=(20,20))
sns.boxplot(x='year_of_release',y='total_sales',data=platform_sales_5, palette='rainbow')
plt.title("Top 5 Platforms Total Sales by Year")
plt.show()


# Looking at the box plot, the year that had the most sales is 2008 with almost 150 sales. There are not a lot of games being sold before 2004, and after 2008 it dropped, making 2016 almost 0. 

# ### Correlation

# In[76]:


PS2 = df[df['platform'] == 'PS2']
PS2


# In[77]:


PS2.corr(method ='kendall')


# In[78]:


PS2.plot(x='critic_score', y='total_sales', kind='scatter') 
plt.title('Critic Score and Total Sale Correlation for PS2')
plt.show()


# In[79]:


PS2.plot(x='user_score', y='total_sales', kind='scatter')
plt.title('User Score and Total Sale Correlation for PS2')
plt.show()


# The correlation, for the platform PS2, between the total sales and critic score is 0.32, while user score and total sale correlation is 0.15. This shows that this is a negligible correlation. Looking at the scatter plots, for both user score and critic score, as the scores go up the totalsales goes up.

# In[80]:


grand_theft = df[df['name'] == 'Grand Theft Auto: San Andreas']
grand_theft


# In[81]:


grand_theft.corr(method ='kendall')


# In[82]:


grand = grand_theft.pivot_table(
    index = 'user_score',
    values = 'total_sales',
    aggfunc = 'sum'
)
print(grand)
grand.plot(kind='bar')
plt.title('User Score and Total Sale Correlation for Grand Theft Auto')
plt.ylabel('Total_sales')


# In[83]:


grand_thef = grand_theft.pivot_table(
    index = 'critic_score',
    values = 'total_sales',
    aggfunc = 'sum'
)
print(grand)
grand.plot(kind='bar')
plt.title('Critic Score and Total Sale Correlation for Grand Theft Auto')
plt.ylabel('Total_sales')


# Picking a random game (Grand Theft Auto: San Andeas), the correlation of the user score and critic score from total_sales was looked at. For critic score and total sales the coorelation is 0.81 which shows a strong correlation. For user score and total sales, the coorelation was 0.67 which shows moderate correlaion. This shows that both have an influance in what someone buys, but the critic score has the strongest influence. 

# ## Genre Sales

# In[84]:


genre_sales = df.groupby(['genre'])['total_sales'].sum().reset_index()
genre_sales


# The genres with the most sales are Action with 1674.27, Sports with 1288.34, Shooter with 987.33, Role-playing with 899.08 and Misc with 788.75. The lowest selling games is puzzles. If we calculate the revenue per published game, Action may not be the most popular genre. 

# In[85]:


df_genre = df[(df['genre'].isin(['Action','Sports','Shooter','Role-Playing','Misc']))]
df_genre


# ### Correlation

# In[86]:


Action = df[df['genre'] == 'Action']
Action


# In[87]:


Action.corr(method ='kendall')


# In[88]:


Action.plot(x='critic_score', y='total_sales', kind='scatter') 
plt.title('Critic Score and Total Sale Correlation for the Genre Action')
plt.show()


# In[89]:


Action.plot(x='user_score', y='total_sales', kind='scatter') 
plt.title('User Score and Total Sale Correlation for the Genre Action')
plt.show()


# There is a 0.29 correlation between critic scores and total sales, and a 0.13 correlation between user score and total sales. This shows that there is a negligible correlation. Looking at the scatter plot, for both critic and user, as the scores go up the total sales go up. 

# ## User Profile For Each Region

# ### Top 5 Platform Region Sales

# In[90]:


df_new


# In[91]:


df_new['na_sales'].sum()


# In[92]:


df_new.groupby(['platform'])['na_sales'].sum()


# In[93]:


df_new['eu_sales'].sum()


# In[94]:


df_new.groupby(['platform'])['eu_sales'].sum()


# In[95]:


df_new['jp_sales'].sum()


# In[96]:


df_new.groupby(['platform'])['jp_sales'].sum()


# In[97]:


df_new['other_sales'].sum()


# In[98]:


df_new.groupby(['platform'])['other_sales'].sum()


# In[149]:


labels = 'na_sales', 'eu_sales', 'jp_sales', 'other_sales'
sizes = [2425.9700000000003, 1374.89, 472.53000000000003, 546.69]
explode = (0.1, 0, 0, 0)  # only "explode" the 1st slice

fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Top 5 Platform Region Sales')
plt.show()


# Comparing the sales of the top 5 platforms across different regions shows that North America has the highest sales. Europe has the second highest and other sales the third highest. Japan had the lowest sales. For North America, X360 was the most popular. For Europe, PS2 was the most popular. For Japan, DS was the most popular. For other slaes, PS2 was the most popular.

# ### Top 5 Genre Region Sales

# In[100]:


df_genre


# In[101]:


df_genre.groupby(['genre'])['na_sales'].sum()


# In[102]:


df_genre['na_sales'].sum()


# In[103]:


df_genre.groupby(['genre'])['eu_sales'].sum()


# In[104]:


df_genre['eu_sales'].sum()


# In[105]:


df_genre.groupby(['genre'])['jp_sales'].sum()


# In[106]:


df_genre['jp_sales'].sum()


# In[107]:


df_genre.groupby(['genre'])['other_sales'].sum()


# In[108]:


df_genre['other_sales'].sum()


# In[148]:


labels = 'na_sales', 'eu_sales', 'jp_sales', 'other_sales'
sizes = [2725.8799999999997, 1579.94, 709.9399999999999, 545.97]
explode = (0.1, 0, 0, 0)  # only "explode" the 1st slice

fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Top 5 Genre Region Sale')
plt.show()


# Looking at different regions for the top 5 genres, North America has the highest sales, then Europe, then Japan, and last Other regions. For North America, Action had the most sales. For Europe Action had the highest sales. For Japan Role-Playing had the highest sales,a nd for other regions Action had the highest sales.

# ### ESRB effect

# In[110]:


df.groupby(['rating'])['na_sales'].sum()


# In[147]:


labels = 'AO', 'E', 'E10+', 'EC', 'K-A', 'M', 'RP', 'T', 'unknown'
sizes = [1.26, 1274.24, 354.50, 1.53, 2.56, 742.87, 0.00, 747.58, 902.99]
explode = (0, 0.1, 0, 0, 0, 0, 0, 0, 0)  # only "explode" the 2nd slice

fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('ESRB Rating for North America')
plt.show()


# In[112]:


df.groupby(['rating'])['eu_sales'].sum()


# In[146]:


labels = 'AO', 'E', 'E10+', 'EC', 'K-A', 'M', 'RP', 'T', 'unknown'
sizes = [0.61, 703.87, 183.33, 0.11, 0.27, 480.00, 0.03, 420.99, 551.33]
explode = (0, 0.1, 0, 0, 0, 0, 0, 0, 0)  # only "explode" the 2nd slice

fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('ESRB Rating for Europe')
plt.show()


# In[114]:


df.groupby(['rating'])['jp_sales'].sum()


# In[144]:


labels = 'AO', 'E', 'E10+', 'EC', 'K-A', 'M', 'RP', 'T', 'unknown'
sizes = [0.00, 197.96, 40.20, 0.00, 1.46, 63.58, 0.00, 150.70, 650.35]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0.1)  # only "explode" the 2nd slice

fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('ESRB Rating for Japan')
plt.show()


# In[116]:


df.groupby(['rating'])['other_sales'].sum()


# In[145]:


labels = 'AO', 'E', 'E10+', 'EC', 'K-A', 'M', 'RP', 'T', 'unknown'
sizes = [0.09, 231.54, 71.92, 0.11, 0.03, 176.06, 0.01, 152.98, 138.08]
explode = (0, 0.1, 0, 0, 0, 0, 0, 0, 0)  # only "explode" the 2nd slice
fig1, ax1 = plt.subplots(figsize=(9,9))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('ESRB Rating for Other Regions')
plt.show()


# Looking at the ratings and the sales for each region with the whole dataset, rating E had the highest sales for North America, Europe and Other regions, but for Japan the highest sales based off of ratings is unknown. If we do not look at the unknown, the next highest is rating E. 

# ## Test statistical hypotheses

# ### Xbox One and PC Platforms

# In[118]:


Xbox = df[df['platform'] == 'XOne']
Xbox


# In[119]:


Xbox = Xbox.dropna(subset = ['user_score'])


# In[120]:


XB = Xbox['user_score']
XB


# In[121]:


XB.mean()


# In[122]:


np.var(XB)


# In[123]:


np.std(XB)


# In[124]:


PC = df[df['platform'] == 'PC']
PC


# In[125]:


PC = PC.dropna(subset = ['user_score'])


# In[126]:


pc = PC['user_score']
pc


# In[127]:


pc.mean()


# In[128]:


np.var(pc)


# In[129]:


np.std(pc)


# Sperate User score from Xbox and PC and find the mean, variation and standard deviation. Use the Series to calculate the hypothesis.

# Hypothesis being tested: Average user ratings of the Xbox One and PC platforms are the same.

# Null Hypothesis: Xbox one and PC platforms have equal user ratings. 
# Alternative Hypothesis:The average user ratings of the Xbox One and the PC Platforms are not equal. 
# I would use a two-tailed hypothesis due to looking in both directions.
# Alpha value is 0.05

# In[130]:


alpha = 0.05
results = stats.ttest_ind(XB, pc, equal_var=False)
print('p-value:',results.pvalue)
if (results.pvalue < alpha):
    print('we reject the null hypthesis')
else:
    print('We cant reject the null hypothesis')


# We reject the null hypothesis, so this means that the average user ratings of the Xbox One and the PC platforms are not equal.

# ### Action and Sport Genres

# In[131]:


Acti = df[df['genre'] == 'Action']
Acti


# In[132]:


Acti = Acti.dropna(subset = ['user_score'])


# In[133]:


action = Acti['user_score']
action


# In[134]:


action.mean()


# In[135]:


np.var(action)


# In[136]:


np.std(action)


# In[137]:


Sport= df[df['genre'] == 'Sports']
Sport


# In[138]:


Sport = Sport.dropna(subset = ['user_score'])


# In[139]:


sports = Sport['user_score']
sports


# In[140]:


sports.mean()


# In[141]:


np.var(sports)


# In[142]:


np.std(sports)


# Hypothesis being tested: Average user ratings for the Action and Sports genres are different

# Null Hypothesis: Action and sports genres have similar user ratings. 
# Alternative Hypothesis:The average user ratings for Action and Sport genres different. 
# I would use a two-tailed hypothesis due to looking in both directions.
# Alpha value is 0.05

# In[143]:


alpha = 0.05
results = stats.ttest_ind(action, sports, equal_var=False)
print('p-value:',results.pvalue)
if (results.pvalue < alpha):
    print('we reject the null hypthesis')
else:
    print('We cant reject the null hypothesis')


# We cant reject the null hypthesis, so that means that Action and Sports genres have similar user ratings. The null hypothesis is the proposal and always stated with an equal sign. The alternative hypothesis is based on the null hypothesis, where the mean of the population does not equal A. The p-value is the probability of getting a result at least as extreme as the one you're considering, assuming that the null hypothesis is correct.The 5% threshold,alpha, (2.5% on each side), is one common measure of statistical significance.

# ## Conclusions 

# After observing the dataframe provided, the game that succeds the most depends on the region. In all the regions, except for Japan, Action games are the most popular. Japans best selling genre was Role-Playing. For platforms, in North America, the X360 was the most popular, PS2 was most popular in Europe and DS was most popular for Japan. For other regions, PS2 was the most popular. But, lookig at the ratings also, rating E is the most popular type of game. Two hypothesis were tested, one was to see if two game systems differ in user ratings (xbox One and PC), and the other was to see if two genres differ in user ratings (Action and Sports). Whn testing the game systems, the xbox one and the PC does not equal eachother, so they have different user rating. Action and Sports have similar user ratings which makes sense since there is negligible corelation between genres. So in conclusion, the most popular game depends on the region.  
